{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layerwise Sequential Unit Variance (LSUV)\n",
    "> A initialization technique for deep architectures\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1511.06422.pdf\"> All You need is a Good Init </a>\n",
    "\n",
    "From the abstract:\n",
    "\n",
    "    Layer-sequential unit-variance (LSUV) initialization – a simple method for weight initialization for deep net learning – is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_08 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Model and Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "nh,bs = 50,512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    Recorder,\n",
    "    partial(AvgStatsCallback, accuracy),\n",
    "    partial(BatchTransformXCallback, mnist_view),\n",
    "    CudaCallback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8,16,32,64,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor our `conv_layer` with two new properties which will make it easier to apply the LSUV algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)\n",
    "        self.relu = GeneralRelu(sub=sub, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "    @property\n",
    "    def bias(self): return -self.relu.sub\n",
    "    \n",
    "    @bias.setter\n",
    "    def bias(self, v): self.relu.sub = -v\n",
    "    \n",
    "    @property\n",
    "    def weight(self): return self.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(data, nfs, ConvLayer, 0.6, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our baseline without initializing the weights with LSUV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.95148234375, tensor(0.3421, device='cuda:0')]\n",
      "valid: [0.791774462890625, tensor(0.7463, device='cuda:0')]\n",
      "train: [0.3390148828125, tensor(0.8941, device='cuda:0')]\n",
      "valid: [0.16294854736328124, tensor(0.9524, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process looks like this:\n",
    "\n",
    "- Initialize neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. \n",
    "- Rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(data, nfs, ConvLayer, 0.4, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get one batch from the dataloader and calls the passed callbacks on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_batch(dl, run):\n",
    "    run.xb, run.yb = next(iter(dl))\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    return run.xb, run.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(learn.data.train_dl, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the model and selecting the linear layers - not Relu or Adaptive Pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_mods(m, func):\n",
    "    if func(m): return [m]\n",
    "    return sum([find_mods(o, func) for o in m.children()], [])\n",
    "\n",
    "def is_lin_layer(l):\n",
    "    layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)\n",
    "    return isinstance(l, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = find_mods(learn.model, lambda o: isinstance(o, ConvLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConvLayer(\n",
       "   (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "   (relu): GeneralRelu()\n",
       " ),\n",
       " ConvLayer(\n",
       "   (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ),\n",
       " ConvLayer(\n",
       "   (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ),\n",
       " ConvLayer(\n",
       "   (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ),\n",
       " ConvLayer(\n",
       "   (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stat(hook, mod, inp, outp):\n",
    "    d = outp.data\n",
    "    hook.mean, hook.std = d.mean().item(), d.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = learn.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08985763788223267 0.2928253710269928\n",
      "0.1276373416185379 0.32792627811431885\n",
      "0.1720675528049469 0.29987233877182007\n",
      "0.14863064885139465 0.24186313152313232\n",
      "0.10681489109992981 0.16747894883155823\n"
     ]
    }
   ],
   "source": [
    "with Hooks(mods, append_stat) as hooks:\n",
    "    mdl(xb)\n",
    "    for hook in hooks: print(hook.mean, hook.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsuv_module(m, xb):\n",
    "    h = Hook(m, append_stat)\n",
    "    \n",
    "    while mdl(xb) is not None and abs(h.mean ) > 1e-3: m.bias -= h.mean\n",
    "    while mdl(xb) is not None and abs(h.std-1) > 1e-3: m.weight.data /= h.std\n",
    "        \n",
    "    h.remove()\n",
    "    return h.mean, h.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2170065939426422, 0.9999998807907104)\n",
      "(-0.036763936281204224, 1.0)\n",
      "(0.11370465904474258, 0.9999999403953552)\n",
      "(0.16291303932666779, 1.0)\n",
      "(0.23237858712673187, 1.0000001192092896)\n"
     ]
    }
   ],
   "source": [
    "for m in mods: print(lsuv_module(m, xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.5101159765625, tensor(0.8371, device='cuda:0')]\n",
      "valid: [0.19917310791015624, tensor(0.9373, device='cuda:0')]\n",
      "train: [0.126850947265625, tensor(0.9605, device='cuda:0')]\n",
      "valid: [0.12543896484375, tensor(0.9638, device='cuda:0')]\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "{\n",
       "const ip = IPython.notebook\n",
       "if (ip) {\n",
       "    ip.save_notebook()\n",
       "    console.log('a')\n",
       "    const s = `!python notebook2script.py ${ip.notebook_name}`\n",
       "    if (ip.kernel) { ip.kernel.execute(s) }\n",
       "}\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_auto_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
