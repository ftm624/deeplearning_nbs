{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Tricks\n",
    "> Refinements and Improvements to CNN's for image classification\n",
    "\n",
    "From the paper: [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)\n",
    "\n",
    "By: Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li\n",
    "\n",
    "    Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_09 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) # downloads and returns a path to folder\n",
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] # transforms to be applied to images\n",
    "\n",
    "il = ImageList.from_files(path, tfms=tfms) # Imagelist from files\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=\"val\")) # Splitdata by function\n",
    "ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcesser()) # label the data by parent folder\n",
    "data = ll.to_databunch(bs, c_in=3, c_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [partial(AvgStatsCallback, accuracy), \n",
    "             CudaCallback,\n",
    "             partial(BatchTransformXCallback, norm_imagenette)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [64,64,128,256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = 2^{\\log_2(x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prev_pow_2(x): return 2**math.floor(math.log2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 8\n",
      "3: 16\n"
     ]
    }
   ],
   "source": [
    "for i in [1,3]:\n",
    "    print(f'{i}:', (2**math.floor(math.log2(i*3*3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that will automatically generate our model layers based on the geometry suggested in the BoT paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cnn_layers(data, nfs, layer, **kwargs):\n",
    "    \n",
    "    def f(ni, nf, stride=2): \n",
    "        return layer(ni, nf, ks=3, stride=stride, **kwargs)\n",
    "    \n",
    "    l1 = data.c_in # channels in from databunch \n",
    "    l2 = prev_pow_2(l1*3*3)\n",
    "    \n",
    "    layers = [f(l1,   l2,   stride=1), # input channels, 2**input channels \n",
    "              f(l2,   l2*2, stride=2),\n",
    "              f(l2*2, l2*4, stride=2)]\n",
    "    nfs = [l2*4] + nfs\n",
    "    \n",
    "    layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)]\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cnn_model(data, nfs, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))\n",
    "\n",
    "def get_learn_run(data, nfs, layer, lr, cbs=None, opt_func=None, uniform=False, **kwargs):\n",
    "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
    "    init_cnn(model, uniform=uniform)\n",
    "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.3, 0.7], cos_1cycle_anneal(0.1, 0.3, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(data, nfs, conv_layer, lr=0.2, cbs=callbacks+[partial(ParamScheduler, 'lr', sched)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.727458206549002, tensor(0.4080, device='cuda:0')]\n",
      "valid: [1.3914296377388535, tensor(0.5299, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that would print out a summary of the layers and their activation shapes of our model would be very helpful. \n",
    "\n",
    "We can do this by using Hooks and sending batch through the model to print out what happens at every stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_summary(run, learn, data, find_all=False):\n",
    "    xb, yb = get_batch(data.valid_dl, run)\n",
    "    device = next(learn.model.parameters()).device\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    hf = lambda hook,mod,inp,outp: print(f'Module:\\n{mod}\\nOutput Shape: {outp.shape}\\n')\n",
    "    mods = find_mods(learn.model, is_lin_layer) if find_all else learn.model.children()\n",
    "    with Hooks(mods, hf) as hook: learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 16, 128, 128])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 32, 64, 64])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 64, 32, 32])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 64, 16, 16])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 64, 8, 8])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 128, 4, 4])\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): GeneralRelu()\n",
      "  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Output:torch.Size([128, 256, 2, 2])\n",
      "\n",
      "AdaptiveAvgPool2d(output_size=1)\n",
      "Output:torch.Size([128, 256, 1, 1])\n",
      "\n",
      "Lambda()\n",
      "Output:torch.Size([128, 256])\n",
      "\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "Output:torch.Size([128, 10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_summary(run, learn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.2597042611086176, tensor(0.5824, device='cuda:0')]\n",
      "valid: [1.4853317824442676, tensor(0.5177, device='cuda:0')]\n",
      "train: [1.1744825012870947, tensor(0.6126, device='cuda:0')]\n",
      "valid: [1.2985995969347133, tensor(0.5809, device='cuda:0')]\n",
      "train: [0.8226048796566428, tensor(0.7270, device='cuda:0')]\n",
      "valid: [1.2384917396496815, tensor(0.6094, device='cuda:0')]\n",
      "train: [0.43220947858637393, tensor(0.8683, device='cuda:0')]\n",
      "valid: [1.1581633160828027, tensor(0.6492, device='cuda:0')]\n",
      "train: [0.18811001638158067, tensor(0.9600, device='cuda:0')]\n",
      "valid: [1.1953391222133758, tensor(0.6530, device='cuda:0')]\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(5, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_auto_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.825px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
